{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Part 0: Installing packages and loading them"
      ],
      "metadata": {
        "id": "SB3pc8E5fy2O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ptU_jrcocq6p"
      },
      "outputs": [],
      "source": [
        "#install required packages not available in colab by default\n",
        "!pip install pdfplumber\n",
        "!pip install python-dotenv\n",
        "!pip install openai\n",
        "!pip install langchain_openai\n",
        "!pip install -U langchain-cli\n",
        "!pip install -qU pinecone-client==3.1.0 pandas==2.0.3\n",
        "!pip install langchain"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#loading libraries\n",
        "import pdfplumber\n",
        "from google.colab import userdata\n",
        "import requests\n",
        "import time\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "from openai import OpenAI\n",
        "import nltk\n",
        "import tiktoken\n",
        "from typing import List\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "mOD5__kbgIRM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part 1: Obtaining data from Old Farmer's Almanac\n",
        "\n",
        "In oder to obtain data from the farmer's Almanac, we first will require a .PDF copy of the Almanac, which can be obtained here:\n",
        "\n",
        "https://store.almanac.com/online-edition-2024\n",
        "\n",
        "The next step is to turn this code into a .txt file which can be ingested."
      ],
      "metadata": {
        "id": "Faiq4X3rcyOp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#convert the farmer's almanac into a .txt file. replace with whatever file name is relevant\n",
        "with pdfplumber.open(\"2024 Old Farmers Almanac.pdf\") as pdf:\n",
        "    text = \"\"\n",
        "    for page in pdf.pages:\n",
        "        text += page.extract_text()\n",
        "with open(\"almanac.txt\", \"w\") as txt:\n",
        "    txt.write(text)"
      ],
      "metadata": {
        "id": "0YApc8z3fgd7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part 1.1 Alternative Data: We could try to get the data from another source like the New York Times. Here we use the API for the new york time to get some information from articles. This could be used instead, though far less successfully, but it could also be used to supplement the Information from the Almanac."
      ],
      "metadata": {
        "id": "-TRdipbUhq4O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#setting the NYT API key. An API key will be required for this. The following code is made to work with colab's \"secrets\" functionality.\n",
        "#if using a different environment (like VS Code) the API key will have to be accessed differently.\n",
        "#either way, it is also possible to simply paste an API key to use.\n",
        "NYT_API_KEY = userdata.get('NYT_API_KEY')\n",
        "\n",
        "#calling the api and saving the results as agriculturedata\n",
        "API_ENDPOINT = 'https://api.nytimes.com/svc/search/v2/articlesearch.json'\n",
        "agriculturedata = ''\n",
        "\n",
        "# Function to fetch articles\n",
        "def fetch_articles(page):\n",
        "    params = {\n",
        "        'q': 'agriculture',\n",
        "        'api-key': NYT_API_KEY,\n",
        "        'page': page,\n",
        "        'fl': 'abstract,snippet,lead_paragraph'  # Requesting abstract, snippet, and lead_paragraph\n",
        "    }\n",
        "    response = requests.get(API_ENDPOINT, params=params)\n",
        "    if response.status_code == 200:\n",
        "        return response.json()\n",
        "    else:\n",
        "        print(f'Error fetching articles: {response.status_code}')\n",
        "        return None\n",
        "\n",
        "# Fetch and concatenate articles\n",
        "for i in range(50):  # 50 pages, 5 articles each to get 250 articles. Note that 500 is daily limit\n",
        "    articles = fetch_articles(i)\n",
        "    if articles:\n",
        "        for article in articles['response']['docs']:\n",
        "            # Concatenate abstract, snippet, and lead_paragraph\n",
        "            if article['abstract']:\n",
        "                agriculturedata += article['abstract'] + '\\n\\n'\n",
        "            if article['snippet']:\n",
        "                agriculturedata += article['snippet'] + '\\n\\n'\n",
        "            if article['lead_paragraph']:\n",
        "                agriculturedata += article['lead_paragraph'] + '\\n\\n'\n",
        "    time.sleep(12)  # Sleep for 12 seconds to respect the rate limit\n",
        "\n",
        "print('Fetched and concatenated articles into agriculturedata.')"
      ],
      "metadata": {
        "id": "_84IU_mbiQ4a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#saving the agriculturedata information in the form of a text document which can be ingested\n",
        "with open('agriculturedata.txt', 'w') as f:\n",
        "    f.write(agriculturedata)"
      ],
      "metadata": {
        "id": "M4jRfSTnja38"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part 2: Implementing the rag model with our Data\n",
        "\n",
        "\n",
        "In this example, we went with the Old Farmer's Almanac, but any data will do. Using more data will probably yield better results, if the data is good.\n",
        "\n",
        "For this to work, an account with pinecone.io is required, and so is an account with openai that has credit balance on it."
      ],
      "metadata": {
        "id": "2Ij2dbZposAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#input the text to be used for this.\n",
        "with open('almanac.txt', 'r') as f:\n",
        "    text = f.read()"
      ],
      "metadata": {
        "id": "tiZbKIxto_4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#if using google colab, this is how to get the api keys from secrets:\n",
        "pc = Pinecone(api_key=userdata.get('PINECONE_API_KEY'))\n",
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "index = pc.Index('agriculture-project')"
      ],
      "metadata": {
        "id": "OOAbHDSYpVdv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the model name for our LLMs. And defining functions for later use\n",
        "OPENAI_MODEL = \"gpt-3.5-turbo\"\n",
        "EMBED_MODEL = \"text-embedding-3-small\"\n",
        "# Store the API key in a variable.\n",
        "\n",
        "client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "MAX_TOKENS = 1536\n",
        "\n",
        "def prep(text: str):\n",
        "    return text.replace(\"\\n\", \" \").replace(\"\\r\", \" \").replace(\"\\t\", \" \")\n",
        "\n",
        "def tokenize(text: List[str]):\n",
        "    encoding = tiktoken.encoding_for_model(EMBED_MODEL)\n",
        "    return encoding.encode(text)\n",
        "\n",
        "def embed(tokens: List[int]):\n",
        "    response = client.embeddings.create(input=tokens,model=EMBED_MODEL)\n",
        "    return response.data[0].embedding\n",
        "\n",
        "def chunk_text(text:str):\n",
        "    current_chunk = []\n",
        "    current_para = \"\"\n",
        "    chunks = []\n",
        "    paras = []\n",
        "    current_len = 0\n",
        "    sentences = nltk.sent_tokenize(text)\n",
        "    chunks_of_tokens = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "        # Tokenize the sentence\n",
        "        sentence_tokens = tokenize(sentence)\n",
        "        sentence_token_len = len(sentence_tokens)\n",
        "\n",
        "        # Check if adding the next sentence exceeds the max token limit\n",
        "        if current_len + sentence_token_len > MAX_TOKENS:\n",
        "            # Add the current chunk to the list and start a new one\n",
        "            paras.append(current_para)\n",
        "            current_para = \"\"\n",
        "            chunks_of_tokens.append(current_chunk)\n",
        "            embeddings = embed(current_chunk)\n",
        "            chunks.append(embeddings)\n",
        "            current_chunk = []\n",
        "            current_len = 0\n",
        "\n",
        "        # Add the sentence to the current chunk\n",
        "        current_para += \" \" + sentence\n",
        "        current_chunk.extend(sentence_tokens)\n",
        "        current_len += sentence_token_len\n",
        "\n",
        "    # Add the last chunk if it's not empty\n",
        "    if current_chunk:\n",
        "        paras.append(current_para)\n",
        "        chunks_of_tokens.append(current_chunk)\n",
        "        embeddings = embed(current_chunk)\n",
        "        chunks.append(embeddings)\n",
        "\n",
        "    return paras, chunks, chunks_of_tokens\n",
        "\n",
        "def create_embeddings(filename: str):\n",
        "    with open(filename, \"r\") as file:\n",
        "        text = file.read()\n",
        "    text = prep(text)\n",
        "    return chunk_text(text)\n",
        "\n",
        "def create_embeddings_prompt(prompt:str):\n",
        "    prompt = prep(prompt)\n",
        "    return chunk_text(prompt)\n",
        "\n",
        "def vectorize_chunks(paras: List, chunks: List, **kwargs):\n",
        "    vectors = []\n",
        "    for i in range(len(chunks)):\n",
        "        if \"filename\" in kwargs:\n",
        "            vectors.append({\"id\": f\"{i}\", \"values\": chunks[i], \"metadata\": {\"file\": filename, \"para\": f\"{paras[i]}\"}})\n",
        "        else:\n",
        "            vectors.append({\"id\": f\"{i}\", \"values\": chunks[i], \"metadata\": {\"para\": f\"{paras[i]}\"}})\n",
        "\n",
        "    return vectors"
      ],
      "metadata": {
        "id": "Xvv2tusdpjih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### takes 30+ minutes runtime - do not use if not necessary\n",
        "# this code provides pinecone.ai with the setup information required. It\n",
        "# creates the vectors which will be stored on pinecone.ai\n",
        "\n",
        "# to prevent inadvertent use of code, replace \"True\" with \"False\" after code has\n",
        "# done running.\n",
        "\n",
        "if(True): #change truth value here\n",
        "    paras, chunks, chunks_of_tokens  = create_embeddings_prompt(text)\n",
        "    vectors = vectorize_chunks(paras, chunks)\n",
        "    print(len(vectors))"
      ],
      "metadata": {
        "id": "_8no0Pikpuze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# upsert the vectors into pinecone index:\n",
        "index.upsert(\n",
        "    vectors=vectors[:200]\n",
        ")\n",
        "index.upsert(\n",
        "    vectors=vectors[200:]\n",
        ")\n",
        "# Note: as of the time of writing this code, the vector list had to be split into two\n",
        "# parts as an error would otherwise occur."
      ],
      "metadata": {
        "id": "D2zJJmh6qs_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part 3: Implementing the Function for asking the questions and Testing"
      ],
      "metadata": {
        "id": "Qy6GXf8ZVUIx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Define Question Asking Function\n",
        "\n",
        "query_responses=[]\n",
        "\n",
        "def ask_a_question(prompt):\n",
        "    # convert the prompt to chunks of  embeddings\n",
        "    paras, chunks, chunks_of_tokens  = create_embeddings_prompt(prompt)\n",
        "    print(f\"Embeddings: {chunks[0]}\")\n",
        "    # vectorize the embeddings\n",
        "    prompt_vectors = vectorize_chunks(paras, chunks)\n",
        "    print(f\"Vectorized: {prompt_vectors[0]}\")\n",
        "    # search the index for the best match using semantic search\n",
        "    query_response = index.query(\n",
        "        top_k=2,\n",
        "        vector=prompt_vectors[0][\"values\"]\n",
        "    )\n",
        "    query_responses.append(query_response)\n",
        "    print(f\"Query response: {query_response}\")\n",
        "    # get the id of the best match\n",
        "    best_id = query_response[\"matches\"][0][\"id\"]\n",
        "    print(f\"Best ID: {best_id}\")\n",
        "    # fetch the best match from the index\n",
        "    result = index.fetch(ids=[best_id])\n",
        "    # get the paragraph of interest from the result metadata\n",
        "    para_of_interest = result[\"vectors\"][best_id][\"metadata\"][\"para\"]\n",
        "    print(f\"Para of interest: {para_of_interest}\")\n",
        "    # Initialize the langchain chat model.\n",
        "    llm = ChatOpenAI(openai_api_key=OPENAI_API_KEY, model_name=OPENAI_MODEL, temperature=0.0)\n",
        "    # turn the para_of_interest into a Document\n",
        "    document = Document(page_content=para_of_interest)\n",
        "    # Create the QA chain using the LLM.\n",
        "    chain = load_qa_chain(llm)\n",
        "    # Pass the para_of_interest and the prompt to the chain, and print the result.\n",
        "    question = \"If you can't find the answer in the provided document, say, I just don't know the answer to that, otherwise, answer the question. \" + prompt\n",
        "    result = chain.invoke({\"input_documents\": [document], \"question\": question})\n",
        "    return result[\"output_text\"]"
      ],
      "metadata": {
        "id": "l5l_J7kITq18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#examples of some questions to ask to ensure that it works:\n",
        "\n",
        "query_responses=[]\n",
        "\n",
        "questions = [\"will there be a solar eclipse?\",\n",
        "            \"what will the weather be like during the summer?\",\n",
        "            \"What are some of the newest developments in farming?\",\n",
        "            \"Will there be a lunar eclipse?\"\n",
        "]\n",
        "\n",
        "answers = []\n",
        "for question in questions:\n",
        "    answers.append(ask_a_question(question))"
      ],
      "metadata": {
        "id": "NxlSQJ9PVFcT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test the clean output function\n",
        "ix = 0\n",
        "for query_response in query_responses:\n",
        "    print(f\"Match Score: {query_response['matches'][0]['score']}\")\n",
        "    print(f\"Question: {questions[ix]}\")\n",
        "    print(f\"Answer:   {answers[ix]}\\n\\n\")\n",
        "    ix += 1"
      ],
      "metadata": {
        "id": "OMb8s6EQV6cz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}